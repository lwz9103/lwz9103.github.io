<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[mlsql-task-7]]></title>
    <url>%2F2021%2F03%2F22%2Fmlsql-task-7%2F</url>
    <content type="text"><![CDATA[任务(7)： 从Python库提取示例数据到Delta Lake中 安装 vega_datasets 1pip install vega_datasets 编写 mlsql, 从 python 库里面获取数据 12345678910111213141516!python conf "schema=st(field(petalLength,double),field(petalWidth,double),field(sepalLength,double),field(sepalWidth,double),field(species,string))";!python conf "dataMode=model";!ray on command '''import plotly.express as pxfrom plotly.io import to_htmlfrom vega_datasets import datadf = data.iris()context.set_output([[df[name] for name in df]])''' named mlsql_temp_table2;select * from mlsql_temp_table2 as output; 理解题 为什么dataMode 设置为 model而不是data。 参考dataMode 详解。 data 是指用 ray 进行分布式计算，数据结果分散在 ray 的各个节点上，mlsql engine 需要根据各个节点信息，去获取结果 123456789101112131415if (dataMode == "data") &#123; val rows = outputDF.collect() val rdd = session.sparkContext.makeRDD[Row](rows, numSlices = rows.length) val stage2_schema_encoder = WowRowEncoder.fromRow(stage2_schema) val newRDD = if (rdd.partitions.length &gt; 0 &amp;&amp; rows.length &gt; 0) &#123; rdd.flatMap &#123; row =&gt; val socketRunner = new SparkSocketRunner("readFromStreamWithArrow", NetUtils.getHost, timezoneID) val commonTaskContext = new SparkContextImp(TaskContext.get(), null) val pythonWorkerHost = row.getAs[String]("host") val pythonWorkerPort = row.getAs[Long]("port").toInt logInfo(s" Ray On Data Mode: connect python worker[$&#123;pythonWorkerHost&#125;:$&#123;pythonWorkerPort&#125;] ") val iter = socketRunner.readFromStreamWithArrow(pythonWorkerHost, pythonWorkerPort, commonTaskContext) iter.map(f =&gt; f.copy()) &#125; &#125; else rdd.map(f =&gt; stage2_schema_encoder(f)) 而 model 则是直接通过 ray client 获取结果 为什么需要设置schema 因为结果是从 python 读回到 mlsql engine 的，mlsql engine 本身一开始并不知道 python 回来的数据会长什么样，所以需要设置 PYTHON_ENV 根据自己需求设置 由于没有安装 conda，所以直接用本机的 python 环境 context.set_output 为什么接受的格式比较特殊？ 大家可以学习下pyarrow.因为 Python 和 Java 之间的数据交换是用 Arrow 做的。Arrow 能省略进程间数据传输序列化反序列的过程，而且能利用 DirectBuffer 实现数据零拷贝。同时，Arrow 的数据存储格式是列式的，所以在数据传输的过程中也是按照一列一列的数据进行传输的。]]></content>
      <categories>
        <category>MLSQL</category>
        <category>MLSQL 入门学习任务</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mlsql-task-6]]></title>
    <url>%2F2021%2F03%2F22%2Fmlsql-task-6%2F</url>
    <content type="text"><![CDATA[任务(6)： 高阶机器学习-从训练到预测 注册模型 1234567891011121314151617181920212223242526272829303132333435363738--NaiveBayesset jsonStr='''&#123;"features":[5.1,3.5,1.4,0.2],"label":0.0&#125;,&#123;"features":[5.1,3.5,1.4,0.2],"label":1.0&#125;&#123;"features":[5.1,3.5,1.4,0.2],"label":0.0&#125;&#123;"features":[4.4,2.9,1.4,0.2],"label":0.0&#125;&#123;"features":[5.1,3.5,1.4,0.2],"label":1.0&#125;&#123;"features":[5.1,3.5,1.4,0.2],"label":0.0&#125;&#123;"features":[5.1,3.5,1.4,0.2],"label":0.0&#125;&#123;"features":[4.7,3.2,1.3,0.2],"label":1.0&#125;&#123;"features":[5.1,3.5,1.4,0.2],"label":0.0&#125;&#123;"features":[5.1,3.5,1.4,0.2],"label":0.0&#125;''';load jsonStr.`jsonStr` as data;select vec_dense(features) as features ,label as label from dataas data1;-- use RandomForesttrain data1 as RandomForest.`/tmp/model` where-- once set true,every time you run this script, MLSQL will generate new directory for you modelkeepVersion="true" -- specicy the test dataset which will be used to feed evaluator to generate some metrics e.g. F1, Accurateand evaluateTable="data1"-- specify group 0 parametersand `fitParam.0.labelCol`="features"and `fitParam.0.featuresCol`="label"and `fitParam.0.maxDepth`="2"-- specify group 1 parametersand `fitParam.1.featuresCol`="features"and `fitParam.1.labelCol`="label"and `fitParam.1.maxDepth`="10";register RandomForest.`/tmp/model` as rf_predict; 批量预测 API 预测]]></content>
      <categories>
        <category>MLSQL</category>
        <category>MLSQL 入门学习任务</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mlsql-task-5]]></title>
    <url>%2F2021%2F03%2F22%2Fmlsql-task-5%2F</url>
    <content type="text"><![CDATA[学习和开发 ET 插件 在 /mlsql/external/mlsql-ets/src/main/java/tech/mlsql/plugins/ets 目录下新增 ET 插件类 KylinSource 编写 KylinSource 123456789101112131415161718192021222324252627package tech.mlsql.plugins.etsimport org.apache.spark.ml.util.Identifiableimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;import org.apache.spark.sql.expressions.UserDefinedFunctionimport streaming.dsl.mmlib.SQLAlgimport streaming.dsl.mmlib.algs.param.WowParamsclass KylinSource(override val uid: String) extends SQLAlg with WowParams &#123; def this() = this(Identifiable.randomUID("tech.mlsql.plugins.ets.KylinSource")) override def train(df: DataFrame, path: String, params: Map[String, String]): DataFrame = &#123; df.sparkSession.read .format("jdbc") .option("url", s"jdbc:kylin://$&#123;params("host")&#125;:$&#123;params("port")&#125;/$&#123;params("project")&#125;") .option("driver", "org.apache.kylin.jdbc.Driver") .option("user", params("user")) .option("password", params("password")) .option("query", params("query")).load() &#125; override def batchPredict(df: DataFrame, path: String, params: Map[String, String]): DataFrame = train(df, path, params) override def load(sparkSession: SparkSession, path: String, params: Map[String, String]): Any = ??? override def predict(sparkSession: SparkSession, _model: Any, name: String, params: Map[String, String]): UserDefinedFunction = ???&#125; 修改 tech.mlsql.plugins.ets.ETApp 注册内置插件，新增一行 1ETRegister.register("Kylin", classOf[KylinSource].getName) 在 streamingpro-mlsql/pom.xml 中新增 kylin jdbc driver 依赖 123456&lt;dependency&gt; &lt;groupId&gt;org.apache.kylin&lt;/groupId&gt; &lt;artifactId&gt;kylin-jdbc&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 启动勾选包含 Provided 的依赖 在简易控制台测试代码]]></content>
      <categories>
        <category>MLSQL</category>
        <category>MLSQL 入门学习任务</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mlsql-task-4]]></title>
    <url>%2F2021%2F03%2F22%2Fmlsql-task-4%2F</url>
    <content type="text"><![CDATA[编写脚本打包发布MLSQL引擎 在工程根目录下创建 package.sh，并赋予执行权限 chmod +x package.sh 编写脚本如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#!/usr/bin/env bashexport LC_ALL=zh_CN.UTF-8export LANG=zh_CN.UTF-8# 依赖的Spark版本export MLSQL_SPARK_VERSION=2.4# scala版本export SCALA_VERSION=2.11# 项目根目录export MLSQL_M_HOME=./# 版本号，保持和maven一致export VERSION=&quot;2.1.0-SNAPSHOT&quot;# 构建export TEMP_DIR=tmp/mlsql-engine_$&#123;MLSQL_SPARK_VERSION&#125;-$&#123;VERSION&#125;export UPLOAD=falseexport ENABLE_CHINESE_ANALYZER=false./dev/package.sh# 在tmp目录生成发行包rm -rf $&#123;TEMP_DIR&#125;mkdir -p $&#123;TEMP_DIR&#125;/libscp streamingpro-mlsql/target/streamingpro-mlsql-spark_$&#123;MLSQL_SPARK_VERSION&#125;_$&#123;SCALA_VERSION&#125;-$&#123;VERSION&#125;.jar $&#123;TEMP_DIR&#125;/libsif [[ &quot;$&#123;ENABLE_CHINESE_ANALYZER&#125;&quot; == &quot;true&quot; ]]; then echo &quot;cp -r lib/*.jar $&#123;TEMP_DIR&#125;/libs/&quot; cp -r lib/*.jar $&#123;TEMP_DIR&#125;/libs/fi## 生成启动脚本等cp dev/start-local.sh $&#123;TEMP_DIR&#125;cat &lt;&lt; EOF &gt; &quot;$&#123;TEMP_DIR&#125;/start-default.sh&quot;if [[ -z &quot;\$&#123;SPARK_HOME&#125;&quot; ]]; then echo &quot;===SPARK_HOME is required===&quot; exit 1fiSELF=\$(cd \$(dirname \$0) &amp;&amp; pwd)cd \$SELF./start-local.shEOFcat &lt;&lt; EOF &gt; &quot;$&#123;TEMP_DIR&#125;/README.md&quot;1. Configure env SPARK_HOME2. Run ./start-default.shEOFchmod u+x $&#123;TEMP_DIR&#125;/*.shcd $&#123;TEMP_DIR&#125; &amp;&amp; cd ..tar czvf mlsql-engine_$&#123;MLSQL_SPARK_VERSION&#125;-$&#123;VERSION&#125;.tar.gz mlsql-engine_$&#123;MLSQL_SPARK_VERSION&#125;-$&#123;VERSION&#125; 在根目录下执行脚本, ./package.sh，在 tmp 目录找到发行包 设置 SPARK_HOME 并启动 MLSQL Engine 1export SPARK_HOME=/Users/wenzheng.liu/devApps/spark-2.4.4-bin-hadoop2.7 &amp;&amp; ./start-default.sh]]></content>
      <categories>
        <category>MLSQL</category>
        <category>MLSQL 入门学习任务</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mlsql-task-1]]></title>
    <url>%2F2021%2F03%2F22%2Fmlsql-task-1%2F</url>
    <content type="text"><![CDATA[设置 MLSQL 开发环境 设置 MLSQL Engine 开发环境 将 MLSQL Engine 项目代码用 IDE 导入，然后设置好 Maven Profile，如下勾选 创建调试启动类，参考如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package streaming.core/** * 2019-03-20 WilliamZhu(allwefantasy@gmail.com) */object WilliamLocalSparkServiceApp &#123; def main(args: Array[String]): Unit = &#123; StreamingApp.main(Array( "-streaming.master", "local[*]", "-streaming.name", "god", "-streaming.rest", "true", "-streaming.thrift", "false", "-streaming.platform", "spark", "-spark.mlsql.enable.runtime.directQuery.auth", "true",// "-streaming.ps.cluster.enable","false", "-streaming.enableHiveSupport","false", "-spark.mlsql.datalake.overwrite.hive", "true", "-spark.mlsql.auth.access_token", "mlsql", //"-spark.mlsql.enable.max.result.limit", "true", //"-spark.mlsql.restful.api.max.result.size", "7", // "-spark.mlsql.enable.datasource.rewrite", "true", // "-spark.mlsql.datasource.rewrite.implClass", "streaming.core.datasource.impl.TestRewrite", //"-streaming.job.file.path", "classpath:///test/init.json", "-streaming.spark.service", "true", "-streaming.job.cancel", "true", "-streaming.datalake.path", "/data/mlsql/datalake", "-streaming.plugin.clzznames","tech.mlsql.plugins.ds.MLSQLExcelApp", // scheduler "-streaming.workAs.schedulerService", "false", "-streaming.workAs.schedulerService.consoleUrl", "http://127.0.0.1:9002", "-streaming.workAs.schedulerService.consoleToken", "mlsql",// "-spark.sql.hive.thriftServer.singleSession", "true", "-streaming.rest.intercept.clzz", "streaming.rest.ExampleRestInterceptor",// "-streaming.deploy.rest.api", "true", "-spark.driver.maxResultSize", "2g", "-spark.serializer", "org.apache.spark.serializer.KryoSerializer",// "-spark.sql.codegen.wholeStage", "true", "-spark.ui.allowFramingFrom","*", "-spark.kryoserializer.buffer.max", "2000m", "-streaming.driver.port", "9003"// "-spark.files.maxPartitionBytes", "10485760" //meta store// "-streaming.metastore.db.type", "mysql",// "-streaming.metastore.db.name", "app_runtime_full",// "-streaming.metastore.db.config.path", "./__mlsql__/db.yml" // "-spark.sql.shuffle.partitions", "1", // "-spark.hadoop.mapreduce.job.run-local", "true" //"-streaming.sql.out.path","file:///tmp/test/pdate=20160809" //"-streaming.jobs","idf-compute" //"-streaming.driver.port", "9005" //"-streaming.zk.servers", "127.0.0.1", //"-streaming.zk.conf_root_dir", "/streamingpro/jack" )) &#125;&#125; 这里需要注意下 streaming.datalake.path，由于本地调试使用的是本地文件系统，需要保证当前用户有权限在此目录下创建文件/目录 创建 MLSQL Console 元数据库 如果没有 mysql 环境，可以用 docker 启动一个 1234docker pull mysql:5.7# 设置数据/日志/配置挂载目录mkdir -p ~/mysql/data ~/mysql/logs ~/mysql/confdocker run -p 3306:3306 --name mysql -v ~/mysql/conf:/etc/mysql/conf.d -v ~/mysql/logs:/logs -v ~/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=root -d mysql:5.7 将 MLSQL Console 代码中的表结构导入 mysql 中 设置 MLSQL Console 开发环境 在 IDE 导入代码后，直接运行 tech.mlsql.MLSQLConsole, 如果遇到配置文件读取不到的问题，可尝试手动设置 Working directory 另外如果启动后，发现无法连通 MLSQL Engine 可在界面手动配置 Engine 的 url]]></content>
      <categories>
        <category>MLSQL</category>
        <category>MLSQL 入门学习任务</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mlsql-task-2]]></title>
    <url>%2F2021%2F03%2F22%2Fmlsql-task-2%2F</url>
    <content type="text"><![CDATA[任务2：学习 MLSQL 基本语言 连接console依赖的数据库中的任意一个表，将数据写到delta lake中，并且查询100条展示出来。 12345678910111213load jdbc.`mlsql_job` where url="jdbc:mysql://localhost:3306/mlsql_console?characterEncoding=utf8&amp;zeroDateTimeBehavior=convertToNull&amp;tinyInt1isBit=false"and driver="com.mysql.jdbc.Driver"and user="root"and password="******"as table1;select * from table1 limit 100 as table2;save append table2 as delta.`dt1`;load delta.`dt1` as table3;select * from table3 as table4; 通过MLSQL mock一些Json格式数据，然后批量写入数据到Kafka中 123456789101112set jsonData='''&#123; "id": 1, "name": "a", "age": 20 ,"gender":"f"&#125;&#123; "id": 2, "name": "b", "age": 25 ,"gender":"m"&#125;&#123; "id": 3, "name": "c", "age": 30 ,"gender":"f"&#125;&#123; "id": 4, "name": "d", "age": 35 ,"gender":"m"&#125;&#123; "id": 5, "name": "e", "age": 19 ,"gender":"f"&#125;''';load jsonStr.`jsonData` as table1;select to_json(struct(*)) as value from table1 as table2;save append table2 as kafka.`aaa` where kafka.bootstrap.servers="127.0.0.1:9092"; (可选题） 写一个流程序参考MLSQL流编程,消费前面写入到Kafka的数据 将数据写入到 delta 中，方便后续消费 12345678910111213141516171819202122-- the stream name, should be uniq.set streamName="kafkaExample";!kafkaTool registerSchema 2 records from "127.0.0.1:9092" aaa;-- convert table as stream sourceload kafka.`aaa` options kafka.bootstrap.servers="127.0.0.1:9092"and failOnDataLoss="false"as newkafkatable1;-- aggregation select * from newkafkatable1as table21;-- output the the result to console.save append table21 as rate.`/tmp/delta/aaa-0` options mode="Append"and idCols="id"and duration="5"and checkpointLocation="/tmp/s-cpl6"; 查看 delta 中的数据 12load delta.`/tmp/delta/aaa-0` as show_table1;select * from show_table1 as output; 在MLSQL中使用Scala完成一个UDF的演示使用例子 利用上方 delta 的数据，写一个分析年龄范围的函数 12345678910111213141516171819202122232425load delta.`/tmp/delta/aaa-0` as show_table1;select * from show_table1 as table2;-- regist udfregister ScriptUDF.`` as ageRangeFun whereand lang="scala"and udfType="udf"and code='''def apply(a:Long)=&#123; var ageRange: String = "" if (a &lt; 20) &#123; ageRange = "1-19" &#125; if (a &gt;= 20 &amp;&amp; a &lt; 30) &#123; ageRange = "20-29" &#125; if (a &gt;= 30) &#123; ageRange = "&gt;=30" &#125; ageRange&#125;''';select *, ageRangeFun(age) from table2 as table3; 离线安装一个excel插件 参考离线安装插件,并上传任意excel文件，然后读取显示出来。 设置好环境后，上传单列 excel 文件，写入代码 123456789101112131415161718192021load binaryFile.`/tmp/upload/report.xlsx` as excel_table;--!python env "PYTHON_ENV=source activate dev";!python conf "schema=st(field(Index,long))";!python conf "runIn=driver";!python conf "dataMode=model";!ray on excel_table '''import ioimport rayfrom pyjava.api.mlsql import RayContextimport pandas as pdray_context = RayContext.connect(globals(),None)excel_file_binary_list = [item for item in RayContext.collect_from(ray_context.data_servers())]df = pd.read_excel(io.BytesIO(excel_file_binary_list[0]["content"]))context.build_result([row for row in df.to_dict('records')])''' named excel_data;]]></content>
      <categories>
        <category>MLSQL</category>
        <category>MLSQL 入门学习任务</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mlsql-task-3]]></title>
    <url>%2F2021%2F03%2F21%2Fmlsql-task-3%2F</url>
    <content type="text"><![CDATA[设置 MLSQL 环境依赖 检查本机 Python 版本 1python 安装 miniconda 12wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.shsh Miniconda3-latest-Linux-x86_64.sh 设置 conda 环境变量 12export CONDA_HOME=$&#123;CONDA_INSTALL_HOME&#125;export PATH=$PATH:$CONDA_HOME/bin 创建 python 虚拟环境并启用 1conda create -n dev python=3.6.7 安装依赖 123456source activate devpip install Cythonpip install pyarrow==0.10.0pip install ray==0.8.0pip install aiohttp psutil setproctitle grpcio pandas xlsxwriterpip install watchdog requests click uuid sfcli pyjava 本机启动 ray 验证代码执行]]></content>
      <categories>
        <category>MLSQL</category>
        <category>MLSQL 入门学习任务</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[开篇语]]></title>
    <url>%2F2019%2F08%2F21%2Ffirst-post%2F</url>
    <content type="text"><![CDATA[为什么开博客？知识管理何为知识管理，个人认为是对过去经历过的或未来可能面临到的各类问题的应对策略的总结，以便在碰到类似问题时高效的找到解决方案。那么，高效的存储和读取知识记录就应该是知识管理工具的必备特点。 之前有用有道云，onenote 做知识记录的习惯，但随着记录经验的增加，发现自己对知识记录有了更进一步的需求。之前只是需要知识的存储检索和同步，现在还需要简洁的记录方式，优雅的外观，同时不失强大的功能。 由于经常使用 github，所以在页面编辑方式上首选 markdown，外观显示上采用 github pages + hexo + next，编辑工具使用 typora。由于 hexo 和 next 有众多开发者贡献插件，各类功能也能得到很好的保障。 大脑瘦身之前经常看一些技术文章，看的过程中觉得理解了。碰到问题的时候，经常会有种感觉，仿佛哪里见过，但就是说不清楚是哪了，怎么解决的。去各类知识平台搜索自己的浏览记录，却发现已经找不到任何痕迹了。这类知识，早被大脑垃圾回收了，也没有持久化，这就基本相当于浪费了阅读这段知识所花的时间。要想让这些知识能重回到大脑里，就得有引用指向这些知识。这个引用的产生可以是从外部知识存储中加载到大脑中，但这存在一定的时限，所以有可能需要定时的加载。另外，日常生活&amp;工作正常进行所需的知识会自然而然的保存到大脑的 context 中，这块知识由于经常被引用，所以能避免被大脑当成垃圾回收掉。 由于大脑存储容量及性能有限，装载的太满容易卡机（垃圾回收频繁，大脑得释放一些内存）。所以对于一些占用空间大，使用频率低的知识可以做外部知识链接，比如各类不常用系统的搭建流程（jenkins，sonar etc.）。外部知识链接即知识存储在外部系统（笔记本），但是脑子里存了一份知识关键字到外部存储位置的链接关系。这里知识的关键字存在不同的详细程度，比如 jenkins 相关的知识、jenkins 搭建相关的流程、jenkins 搭建中构建节点的配置，其详细程度是依次递增的。对一个知识认识的详细程度越高，搜索起来效率也越高（搜索关键字准确），trade off 是需要更多的时间和空间来加载这些知识链接。 系统思考写作是系统化思考知识最有效的方式。平时，可能看到或者听来的知识，可能很少来得及深入思考，这就导致较复杂的一些知识没法有效的转换成自己的经验。但是，在写作总结知识的过程中，脑子里会抛出一系列疑问和假设，对这些疑问和假设一一思考过后，就会更加深刻的理解知识其中的含义，写出来的东西也更能让未来的你或者别人看懂。 写博客需要注意的点由浅入深，适可而止知识的整理存档并不是一件轻松的事情，所以需要衡量效用和耗时。我把对知识的理解归为三个层次。第一层次，能够正确使用知识暴露的最外层接口。第二层次，理解知识内在的工作逻辑，在出现异常状况时，能够快速找到 Root Cause。第三层级，深刻理解知识运用的场景，明白其潜在的缺陷，并且能有自己的见解去改善它。 大部分时候，我们只需要掌握第一层次，就能基本应付工作和生活了。所以在去深入了解一门知识前，都先考虑考虑是否值得花这些时间。 从源头收集资料从源头收集资料，这样可以避免多次转手带来的偏差。参考的优先级为： 官网/国际期刊论文书籍 --&gt; 卓有成就的专家博客 --&gt; 较专业的知识分享网站 --&gt; google 搜索 --&gt; bing --&gt; 某度 经过验证知识存档前需要验证其准确性。有些知识可能会因为来源问题，时间问题（版本）导致不可被验证。这类需要被剔除，因为它已经没用了。]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>扯淡</tag>
      </tags>
  </entry>
</search>
